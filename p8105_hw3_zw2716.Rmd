---
title: "Homework 3"
author: "Iris (Ziyi) Wang"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
#library(rnoaa)
library(ggridges)
library(patchwork)
#devtools::install_github("p8105/p8105.datasets")
library(p8105.datasets)

knitr::opts_chunk$set(
    fig.width = 6, 
    fig.asp = .6,
    out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

### Due date

Due: October 10 at 10:00pm. 

### Points

| Problem         | Points    |
|:--------------- |:--------- |
| Problem 0       | 20        |
| Problem 1       | --        |
| Problem 2       | 40        |
| Problem 3       | 40        |
| Optional survey | No points |


### Problem 0

This "problem" focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files. 



### Problem 1

```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)`  columns. 

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```


Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```


### Problem 2

#### Load, tidy, and otherwise wrangle the accelerometer data. 
```{r accelerometer, message = F}
path_to_P2_data = "./data/accel_data.csv"
accelerometer = read_csv(path_to_P2_data) %>% 
    janitor::clean_names() %>% 
    pivot_longer(
        cols = starts_with("activity_"),
        names_to = 'activity_number',
        values_to = "activity_minute",
        names_prefix = "activity_") %>% 
    mutate(
        activity_minute = as.numeric(activity_minute),
        activity_number = as.numeric(activity_number),
        day = factor(day),
        # add weekday_vs_weekend variable
        week_end_vs_day = ifelse(day_id == c(3,4), "weekend", "weekday")
    )
skimr::skim_without_charts(accelerometer)
```

After tidy and wrangle the data, the resulting _**accelerometer**_ data contains `r nrow(accelerometer)` rows and `r ncol(accelerometer)` columns. The variables in the _**accelerometer**_ are: `r names(accelerometer)`. Specifically, **weekday_vs_weekend** shows whether the day is a weekday or weekend; **activity_minute** and **activity_number** shows the minute for each activities.

#### Traditional analyses of accelerometer data 
```{r total_activity, warning = F, message = F}
# aggregate activity_minute to get total_activity var. for each day
accelerometer %>% 
    group_by(week,day) %>% 
    summarize(total_activity = sum(activity_minute)) %>% 
    # create a table showing these totals
    pivot_wider(names_from = day,
                values_from = total_activity) %>% 
    knitr::kable()
```

Apparent trend I noticed is that week 4 and 5's Saturday both have the exact same amount of low activity, so I am a little curious whether that two numbers are resulted from reading error or technical error, or the person just happen to move less and has the same amount of activity. Another potential trend is that this person has relatively the same amount of activity on Wednesdays and Thursdays.

#### single-panel plot: 24-hour activity time courses for each day
```{r single-panel plot, warning = F, message = F}
accelerometer %>% 
    ggplot(aes(x = activity_number, 
               y = activity_minute, 
               color = day, group = day_id)) +
    geom_smooth(se = F, method = "loess", geom = "line", alpha = 0.5) +
    labs(title = "24-hour activity time courses for each day",
         x = "Time in a day (in minutes)",
         y = "Activity Count") +
    guides(color = guide_legend(nrow = 1, byrow = T))
```
Based on the above graph, I see the person has more activity during the (middle of the) day, and less/no activity during the early morning and late night. This makes sense as people generally asleep during the early morning and late night, and work during the (middle of the) day. In addition, this person seems to very low (almost 0) activities on one Saturday and Monday, maybe this is due to the person did not (forgot to) put on the accelerometer as him/she started the day.

### Problem 3
```{r load noaa data}
data("ny_noaa")
skimr::skim_without_charts(ny_noaa)
```
The _**ny_noaa**_ shows New York state's weather data (precipitation, snowfall, temperature) from 1981-01-01 to 2010-12-31. _**ny_noaa**_ has `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The key variables are `r names(ny_noaa)`.

###### Detailed explanation for each variables: 
* id: Weather station ID
* date: Date of observation
* prcp: Precipitation (tenths of mm)
* snow: Snowfall (mm)
* snwd: Snow depth (mm)
* tmax: Maximum temperature (tenths of degrees C)
* tmin: Minimum temperature (tenths of degrees C)

As we can see from the above information, there are a huge amount of missing data (by looking at the n_missing fields), which will be an issue for data exploration and analysis.

#### Data Cleaning
```{r load_noaa}
ny_noaa_tidy = ny_noaa %>% 
    separate(date, 
             into = c("year","month","day"), 
             sep = "-") %>% 
    mutate_at(vars(year,month,day), as.factor) %>% 
    mutate_at(vars(tmax, tmin, prcp, snow, snwd), as.numeric) %>% 
    mutate(tmax = tmax / 10,
           tmin = tmin / 10,
           prcp = prcp / 10)

# snowfall, find the most commonly obs val
ny_noaa_tidy %>% 
    count(snow) %>%
    arrange(desc(n))
```
For snowfall, the most commonly observed value is **0**.  
This makes sense as snow usually happens around winter time in New York, so during most of the year there will not be snow. The second commonly observed is **NA** (missing value), then the third commonly observed is **25**.

#### Two-panel plot for the average max temperature in January and in July in each station across years. 

```{r max temp, warning = F, message = F}
ny_noaa_tidy %>%
    filter(month %in% c("01", "07")) %>%
    group_by(id,month,year) %>% 
    summarise(avg_temp = mean(tmax, na.rm = T)) %>%
    mutate(month = ifelse( month == "01", "January", "July")) %>% 
    drop_na() %>%
    ggplot(aes(x = year, y = avg_temp)) +
    geom_point(aes(color = month)) + 
    geom_smooth() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    facet_grid(.~month ) +
    labs(title = "Average Max Temperature in January and in July in each station across years",
        x = "Year", y = "Average Max Tempature (C)")
```
Is there any observable / interpretable structure? Any outliers? 
From the two-panel plot plots above we can see that the average max temperature for January is much lower then the average max temperature for July. This makes sense, as in New York, summer is in the July, which is much hotter than the winter in January.  
Throughout 1981-2010, we can see an overall trend that the average max temperature for January has wider variation (range from -10C to 10C) than average max temperature for July (range from 20C to 35C). 
In terms of outliers, both January and July has some outliers for average max temperature, this might due to measuring error, reading error, or an actually extreme temperature event.

