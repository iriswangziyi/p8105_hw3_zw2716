Homework 3
================
Iris (Ziyi) Wang

### Due date

Due: October 10 at 10:00pm.

### Points

| Problem         | Points    |
| :-------------- | :-------- |
| Problem 0       | 20        |
| Problem 1       | –         |
| Problem 2       | 40        |
| Problem 3       | 40        |
| Optional survey | No points |

### Problem 0

This “problem” focuses on structure of your submission, especially the
use git and GitHub for reproducibility, R Projects to organize your
work, R Markdown to write reproducible reports, relative paths to load
data from local files, and reasonable naming structures for your files.

### Problem 1

``` r
data("instacart")
```

This dataset contains 1384617 rows and 15 columns.

Observations are the level of items in orders by user. There are user /
order variables – user ID, order ID, order day, and order hour. There
are also item variables – name, aisle, department, and some numeric
codes.

How many aisles, and which are most items from?

``` r
instacart %>% 
    count(aisle) %>% 
    arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # … with 124 more rows

Let’s make a plot

``` r
instacart %>% 
    count(aisle) %>% 
    filter(n > 10000) %>% 
    mutate(
        aisle = factor(aisle),
        aisle = fct_reorder(aisle, n)
    ) %>% 
    ggplot(aes(x = aisle, y = n)) + 
    geom_point() + 
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw3_zw2716_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

Let’s make a table\!\!

``` r
instacart %>% 
    filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
    group_by(aisle) %>% 
    count(product_name) %>% 
    mutate(rank = min_rank(desc(n))) %>% 
    filter(rank < 4) %>% 
    arrange(aisle, rank) %>% 
    knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Apples vs ice cream..

``` r
instacart %>% 
    filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
    group_by(product_name, order_dow) %>% 
    summarize(mean_hour = mean(order_hour_of_day)) %>% 
    pivot_wider(
        names_from = order_dow,
        values_from = mean_hour
    )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

### Problem 2

#### Load, tidy, and otherwise wrangle the accelerometer data.

``` r
path_to_P2_data = "./data/accel_data.csv"
accelerometer = read_csv(path_to_P2_data) %>% 
    janitor::clean_names() %>% 
    pivot_longer(
        cols = starts_with("activity_"),
        names_to = 'activity_number',
        values_to = "activity_minute",
        names_prefix = "activity_") %>% 
    mutate(
        activity_minute = as.numeric(activity_minute),
        activity_number = as.numeric(activity_number),
        day = factor(day),
        # add weekday_vs_weekend variable
        week_end_vs_day = ifelse(day_id == c(3,4), "weekend", "weekday")
    )
skimr::skim_without_charts(accelerometer)
```

|                                                  |               |
| :----------------------------------------------- | :------------ |
| Name                                             | accelerometer |
| Number of rows                                   | 50400         |
| Number of columns                                | 6             |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |               |
| Column type frequency:                           |               |
| character                                        | 1             |
| factor                                           | 1             |
| numeric                                          | 4             |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |               |
| Group variables                                  | None          |

Data summary

**Variable type: character**

| skim\_variable     | n\_missing | complete\_rate | min | max | empty | n\_unique | whitespace |
| :----------------- | ---------: | -------------: | --: | --: | ----: | --------: | ---------: |
| week\_end\_vs\_day |          0 |              1 |   7 |   7 |     0 |         2 |          0 |

**Variable type: factor**

| skim\_variable | n\_missing | complete\_rate | ordered | n\_unique | top\_counts                                |
| :------------- | ---------: | -------------: | :------ | --------: | :----------------------------------------- |
| day            |          0 |              1 | FALSE   |         7 | Fri: 7200, Mon: 7200, Sat: 7200, Sun: 7200 |

**Variable type: numeric**

| skim\_variable   | n\_missing | complete\_rate |   mean |     sd | p0 |    p25 |   p50 |     p75 | p100 |
| :--------------- | ---------: | -------------: | -----: | -----: | -: | -----: | ----: | ------: | ---: |
| week             |          0 |              1 |   3.00 |   1.41 |  1 |   2.00 |   3.0 |    4.00 |    5 |
| day\_id          |          0 |              1 |  18.00 |  10.10 |  1 |   9.00 |  18.0 |   27.00 |   35 |
| activity\_number |          0 |              1 | 720.50 | 415.70 |  1 | 360.75 | 720.5 | 1080.25 | 1440 |
| activity\_minute |          0 |              1 | 267.04 | 443.16 |  1 |   1.00 |  74.0 |  364.00 | 8982 |

After tidy and wrangle the data, the resulting ***accelerometer*** data
contains 50400 rows and 6 columns. The variables in the
***accelerometer*** are: week, day\_id, day, activity\_number,
activity\_minute, week\_end\_vs\_day.  
Specifically, **weekday\_vs\_weekend** shows whether the day is a
weekday or weekend; **activity\_minute** and **activity\_number** shows
the minute for each activities.

#### Traditional analyses of accelerometer data

``` r
# aggregate activity_minute to get total_activity var. for each day
accelerometer %>% 
    group_by(week,day) %>% 
    summarize(total_activity = sum(activity_minute)) %>% 
    # create a table showing these totals
    pivot_wider(names_from = day,
                values_from = total_activity) %>% 
    knitr::kable()
```

| week |   Friday |    Monday | Saturday | Sunday | Thursday |  Tuesday | Wednesday |
| ---: | -------: | --------: | -------: | -----: | -------: | -------: | --------: |
|    1 | 480542.6 |  78828.07 |   376254 | 631105 | 355923.6 | 307094.2 |    340115 |
|    2 | 568839.0 | 295431.00 |   607175 | 422018 | 474048.0 | 423245.0 |    440962 |
|    3 | 467420.0 | 685910.00 |   382928 | 467052 | 371230.0 | 381507.0 |    468869 |
|    4 | 154049.0 | 409450.00 |     1440 | 260617 | 340291.0 | 319568.0 |    434460 |
|    5 | 620860.0 | 389080.00 |     1440 | 138421 | 549658.0 | 367824.0 |    445366 |

Apparent trend I noticed is that week 4 and 5’s Saturday both have the
exact same amount of low activity, so I am a little curious whether that
two numbers are resulted from reading error or technical error, or the
person just happen to move less and has the same amount of activity.  
Another potential trend is that this person has relatively the same
amount of activity on Wednesdays and Thursdays.

#### single-panel plot: 24-hour activity time courses for each day

``` r
accelerometer %>% 
    ggplot(aes(x = activity_number, 
               y = activity_minute, 
               color = day, group = day_id)) +
    geom_smooth(se = F, method = "loess", geom = "line", alpha = 0.5) +
    labs(title = "24-hour activity time courses for each day",
         x = "Time in a day (in minutes)",
         y = "Activity Count") +
    guides(color = guide_legend(nrow = 1, byrow = T))
```

<img src="p8105_hw3_zw2716_files/figure-gfm/single-panel plot-1.png" width="90%" />

Based on the above graph, I see the person has more activity during the
(middle of the) day, and less/no activity during the early morning and
late night. This makes sense as people generally asleep during the early
morning and late night, and work during the (middle of the) day. In
addition, this person seems to very low (almost 0) activities on one
Saturday and Monday, maybe this is due to the person did not (forgot to)
put on the accelerometer as him/she started the day.

### Problem 3

``` r
data("ny_noaa")
skimr::skim_without_charts(ny_noaa)
```

|                                                  |          |
| :----------------------------------------------- | :------- |
| Name                                             | ny\_noaa |
| Number of rows                                   | 2595176  |
| Number of columns                                | 7        |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_   |          |
| Column type frequency:                           |          |
| character                                        | 3        |
| Date                                             | 1        |
| numeric                                          | 3        |
| \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ |          |
| Group variables                                  | None     |

Data summary

**Variable type: character**

| skim\_variable | n\_missing | complete\_rate | min | max | empty | n\_unique | whitespace |
| :------------- | ---------: | -------------: | --: | --: | ----: | --------: | ---------: |
| id             |          0 |           1.00 |  11 |  11 |     0 |       747 |          0 |
| tmax           |    1134358 |           0.56 |   1 |   4 |     0 |       532 |          0 |
| tmin           |    1134420 |           0.56 |   1 |   4 |     0 |       548 |          0 |

**Variable type: Date**

| skim\_variable | n\_missing | complete\_rate | min        | max        | median     | n\_unique |
| :------------- | ---------: | -------------: | :--------- | :--------- | :--------- | --------: |
| date           |          0 |              1 | 1981-01-01 | 2010-12-31 | 1997-01-21 |     10957 |

**Variable type: numeric**

| skim\_variable | n\_missing | complete\_rate |  mean |     sd |   p0 | p25 | p50 | p75 |  p100 |
| :------------- | ---------: | -------------: | ----: | -----: | ---: | --: | --: | --: | ----: |
| prcp           |     145838 |           0.94 | 29.82 |  78.18 |    0 |   0 |   0 |  23 | 22860 |
| snow           |     381221 |           0.85 |  4.99 |  27.22 | \-13 |   0 |   0 |   0 | 10160 |
| snwd           |     591786 |           0.77 | 37.31 | 113.54 |    0 |   0 |   0 |   0 |  9195 |

The ***ny\_noaa*** shows New York state’s weather data (precipitation,
snowfall, temperature) from 1981-01-01 to 2010-12-31. ***ny\_noaa*** has
2595176 rows and 7 columns. The key variables are id, date, prcp, snow,
snwd, tmax, tmin.

###### Detailed explanation for each variables:

  - id: Weather station ID
  - date: Date of observation
  - prcp: Precipitation (tenths of mm)
  - snow: Snowfall (mm)
  - snwd: Snow depth (mm)
  - tmax: Maximum temperature (tenths of degrees C)
  - tmin: Minimum temperature (tenths of degrees C)

As we can see from the above information, there are a huge amount of
missing data (by looking at the n\_missing fields), which will be an
issue for data exploration and analysis.

#### Data Cleaning

``` r
ny_noaa_tidy = ny_noaa %>% 
    separate(date, 
             into = c("year","month","day"), 
             sep = "-") %>% 
    mutate_at(vars(year,month,day), as.factor) %>% 
    mutate_at(vars(tmax, tmin, prcp, snow, snwd), as.numeric) %>% 
    mutate(tmax = tmax / 10,
           tmin = tmin / 10,
           prcp = prcp / 10)

# snowfall, find the most commonly obs val
ny_noaa_tidy %>% 
    count(snow) %>%
    arrange(desc(n))
```

    ## # A tibble: 282 x 2
    ##     snow       n
    ##    <dbl>   <int>
    ##  1     0 2008508
    ##  2    NA  381221
    ##  3    25   31022
    ##  4    13   23095
    ##  5    51   18274
    ##  6    76   10173
    ##  7     8    9962
    ##  8     5    9748
    ##  9    38    9197
    ## 10     3    8790
    ## # … with 272 more rows

For snowfall, the most commonly observed value is **0**.  
This makes sense as snow usually happens around winter time in New York,
so during most of the year there will not be snow. The second commonly
observed is **NA** (missing value), then the third commonly observed is
**25**.

#### Two-panel plot for the average max temperature in January and in July in each station across years.

``` r
ny_noaa_tidy %>%
    filter(month %in% c("01", "07")) %>%
    group_by(id,month,year) %>% 
    summarise(avg_temp = mean(tmax, na.rm = T)) %>%
    mutate(month = ifelse( month == "01", "January", "July")) %>% 
    drop_na() %>%
    ggplot(aes(x = year, y = avg_temp)) +
    geom_point(aes(color = month)) + 
    geom_smooth() +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    facet_grid(.~month ) +
    labs(title = "Average Max Temperature in January and in July in each station across years",
        x = "Year", 
        y = "Average Max Tempature (C)")
```

<img src="p8105_hw3_zw2716_files/figure-gfm/max temp-1.png" width="90%" />

From the two-panel plot plots above we can see that the average max
temperature for January is much lower then the average max temperature
for July. This makes sense, as in New York, summer is in the July, which
is much hotter than the winter in January.

Throughout 1981-2010, we can see an overall trend that the average max c
for January has wider variation (range from -10C to 10C) than average
max temperature for July (range from 20C to 35C). In terms of outliers,
both January and July has some outliers for average max temperature,
this might due to measuring error, reading error, or an actually extreme
temperature event.

#### Two-panel plot showing (i) tmax vs tmin for the full dataset; and (ii) distribution of snowfall values \> 0 and \< 100 separately by year

``` r
tmax_vs_tmin = ny_noaa_tidy %>%
    ggplot(aes(x = tmax, y = tmin)) + 
    geom_bin2d() +
    labs(title = "Min vs Max Tempatures",
         x = "Max temperature (C)",
         y = "Min temperature (C)") 
```

``` r
dist_snow = ny_noaa_tidy %>%
    filter(snow > 0 & snow < 100) %>%
    group_by(year) %>%
    ggplot(aes(x = year, y = snow)) +
    geom_boxplot() +
    stat_summary(fun = "median", color = "blue") +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
    labs(title = "Distribution of Snowfall",
         x = "Years", 
         y = "Snowfall (mm)") 
```

``` r
(tmax_vs_tmin / dist_snow)
```

<img src="p8105_hw3_zw2716_files/figure-gfm/final two-panel plot-1.png" width="90%" />

From the top plot in our two-panel plot, we can see that the min and max
temperatures is centered in the middle of the plot with two bright
yellow box showing the most commonly observed values (min at 0C and 20C;
max at 5C and 25C).

There are also many overlapping values for min and max temperatures.
From the bottom plot in our two-panel plot, we can see that the
distribution of snowfall has consistent/similar mean and IQR since 1982
to 2010, which is quite surprising to me. I am curious about any reason
behinds it.
